# 인공지능 2주차-1

추가 일시: 2024년 7월 8일 오후 3:30
강의: AI-summer

### Activation Function

### 활성 함수란?

활성 함수는 신경망의 각 뉴런 출력에 적용되는 수학적 함수로, 모델에 비선형성을 도입하여 복잡한 데이터 패턴을 학습할 수 있도록 하는 것이 목적이다.

### 활성화 함수의 목적

1. **비선형성 도입**: 모델이 복잡한 패턴을 학습할 수 있도록 비선형 특성을 도입한다. XOR같은 경우 선형으로 표현이 불가능하기에 비선형 함수를 도입이 필요하다.
2. **값의 매핑**: 입력값을 특정 범위(주로 0~1 또는 -1~1)로 매핑하여, 경사하강법과 같은 최적화 방법을 더 잘 수행할 수 있게 한다.

### 종류와 특징

1. **Sigmoid (시그모이드)**
    - **공식**: sigmoid(x)=1/(1+exp(x))
    - **출력 범위**: (0, 1)
    - **특징**:
        - 부드러운 경사
        - 출력 값이 0과 1 사이로 제한됨
        - 출력이 확률인 모델에서 유용
        - 기울기 소실(gradient vanishing) 문제 발생 가능
        
        ![Untitled](https://github.com/IMS-STUDY/2024_Summer_AI_Study/assets/127017020/93135356-9162-42bb-895d-208edfc7a58b)
        
2. **Tanh (하이퍼볼릭 탄젠트)**
    - **공식**: tanh(x)=(exp(x)-exp(-x))/(exp(x)+exp(-x))
    - **출력 범위**: (-1, 1)
    - **특징**:
        - 부드러운 경사
        - 0을 중심으로 함, 최적화가 더 쉬움
        - 입력이 0에 가까운 경우 더 강하게 출력
        - 시그모이드보다 기울기 소실 문제 덜 발생
    
    ![Untitled 1](https://github.com/IMS-STUDY/2024_Summer_AI_Study/assets/127017020/99a4b5a0-419e-473a-9e62-907ac233fa76)
    
3. **ReLU (렐루)**
    - **공식**: ReLU(x)=max(0,x) → 0보다 작으면 0 크면 x
    - **출력 범위**: [0, ∞)
    - **특징**:
        - 간단하고 계산 효율적
        - 기울기 소실 문제를 줄임
        - 음수 입력에 대해 출력을 0으로 설정하여 희소성 도입
        - 기울기 소실 대신 기울기 폭발 문제 발생 가능
        
        ![Untitled 2](https://github.com/IMS-STUDY/2024_Summer_AI_Study/assets/127017020/975aa23c-44b1-438f-a604-0f93bd09c6dc)
        
4. **Softmax (소프트맥스)**
    - **공식**: softmax(xi)=exp(xi)/sum(exp(x))
    - **출력 범위**: (0, 1), 출력 값의 합이 1
    - **특징**:
        - 다중 클래스 분류 문제에 사용
        - 각 클래스에 대한 확률을 제공
        - 출력 값의 합이 1이 되도록 함
        - 입력을 여러개 받을 수 있음
        - 입력이 한개일경우 sigmoid와 차이없음
    
    ![Untitled 3](https://github.com/IMS-STUDY/2024_Summer_AI_Study/assets/127017020/b3c777a1-8563-43f5-8a47-87576491d197)

기울기 손실: 기울기가 매우작아져 학습하지 못하는 경우 발생 

기울기 폭발: 기울기 폭발은 기울기가 학습에 방해될 만큼 커질 때 발생

### K-Fold Cross-Validation

### K-Fold란?

K-Fold 교차 검증은 데이터를 K개의 폴드로 나누어 모델을 학습하고 평가하는 방법입니다. 이는 데이터의 모든 부분이 한 번씩 검증 세트로 사용되도록 하여 모델의 일반화 성능을 평가하는 데 사용된다.

### K-Fold 원리

1. **데이터 분할**: 데이터를 K개의 폴드로 나눈다. 이때 폴드 하나의 크기는 데이터의 크기를 K로 나눈 값이다.
2. **모델 학습 및 평가**:
    - N 번째 폴드(N의 시작은 1)를 검증 세트로 사용하고, 나머지 K-1개의 폴드를 학습 세트로 사용하여 모델을 학습한다.
    - 검증 세트로 모델의 정확도를 확인한다.
    - 이 과정을 N을 1씩 증가시키며 K번 반복하여, 각 폴드가 한 번씩 검증 세트로 사용된다.
3. **결과 통합**: K번의 평가 결과를 평균내어 모델의 정확도를 구한다.

### K-Fold의 장점

- **더 나은 일반화 평가**: 모든 데이터가 학습과 검증에 사용되므로 모델의 일반화 성능을 더 정확하게 평가할 수 있다.
- **편향 감소**: 단일 훈련-검증 분할에서 발생할 수 있는 편향을 줄인다.