{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = r\"C:\\Users\\narea\\Desktop\\Dataset\"\n",
    "def findFiles(path): return glob.glob(path)\n",
    "#데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "#random.seed(777)\n",
    "#torch.manual_seed(777)\n",
    "#if device == 'cuda':\n",
    "#    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터 설정\n",
    "training_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 2500    # 300x300 image       # 고정된 값 (이미지 크기)\n",
    "hidden_size = 1000   # 임의의 값           # 임의의 값 (hidden layer의 노드 개수)\n",
    "output_size = 2    # OX               # 고정된 값 (분류할 클래스 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 배열 크기 : 280\n",
      "첫번째 원소값 사이즈: torch.Size([1, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image_list1 = [os.path.join(path, fname) for fname in os.listdir(path) if fname.endswith('.png')]\n",
    "image_list2 = [os.path.join(path, fname) for fname in os.listdir(path) if fname.endswith('.jpg')]\n",
    "\n",
    "image_list = image_list1+image_list2\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "#image = Image.open(image_list[0]).convert(\"L\")\n",
    "#image = transform(image)\n",
    "\n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "for idx in image_list:\n",
    "    image = cv2.imread(idx, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (50,50))\n",
    "    image = Image.fromarray(image)\n",
    "    image = transform(image)\n",
    "    dataset.append(image)\n",
    "\n",
    "    filename = os.path.basename(idx).lower()\n",
    "    if 'o' in filename:\n",
    "        labels.append(0) #결과 레이블 'o'\n",
    "    elif 'x' in filename:\n",
    "        labels.append(1) #결과 레이블 'x'\n",
    "\n",
    "print(f\"dataset 배열 크기 : {len(dataset)}\")\n",
    "print(f\"첫번째 원소값 사이즈: {dataset[0].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트를 텐서로 변환\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(list(zip(dataset, labels)), [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Flatten(),  # 50*50 이미지를 250*1 벡터로 변환\n",
    "            nn.Linear(in_features=2500, out_features=1250, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1250, out_features=625, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=625, out_features=315, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=315, out_features=150, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=150, out_features=70, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=70, out_features=2, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss() #손실함수\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #옵티마이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.692518055\n",
      "Epoch: 0002 cost = 0.691993654\n",
      "Epoch: 0003 cost = 0.693644404\n",
      "Epoch: 0004 cost = 0.692798793\n",
      "Epoch: 0005 cost = 0.690797865\n",
      "Epoch: 0006 cost = 0.689669430\n",
      "Epoch: 0007 cost = 0.687965095\n",
      "Epoch: 0008 cost = 0.683835864\n",
      "Epoch: 0009 cost = 0.680390239\n",
      "Epoch: 0010 cost = 0.679513931\n",
      "Epoch: 0011 cost = 0.668823242\n",
      "Epoch: 0012 cost = 0.663470984\n",
      "Epoch: 0013 cost = 0.675972283\n",
      "Epoch: 0014 cost = 0.653896034\n",
      "Epoch: 0015 cost = 0.633952439\n",
      "Epoch: 0016 cost = 0.607766926\n",
      "Epoch: 0017 cost = 0.587713480\n",
      "Epoch: 0018 cost = 0.551999688\n",
      "Epoch: 0019 cost = 0.564672709\n",
      "Epoch: 0020 cost = 0.574575007\n",
      "Epoch: 0021 cost = 0.487604618\n",
      "Epoch: 0022 cost = 0.535993278\n",
      "Epoch: 0023 cost = 0.554799795\n",
      "Epoch: 0024 cost = 0.543989956\n",
      "Epoch: 0025 cost = 0.699336231\n",
      "Epoch: 0026 cost = 0.528598011\n",
      "Epoch: 0027 cost = 0.527323544\n",
      "Epoch: 0028 cost = 0.507255137\n",
      "Epoch: 0029 cost = 0.457502961\n",
      "Epoch: 0030 cost = 0.457704663\n",
      "Epoch: 0031 cost = 0.449349284\n",
      "Epoch: 0032 cost = 0.467080981\n",
      "Epoch: 0033 cost = 0.403143018\n",
      "Epoch: 0034 cost = 0.403527677\n",
      "Epoch: 0035 cost = 0.446242213\n",
      "Epoch: 0036 cost = 0.524682879\n",
      "Epoch: 0037 cost = 0.547206104\n",
      "Epoch: 0038 cost = 0.435310394\n",
      "Epoch: 0039 cost = 0.379340053\n",
      "Epoch: 0040 cost = 0.384536713\n",
      "Epoch: 0041 cost = 0.359859556\n",
      "Epoch: 0042 cost = 0.412120581\n",
      "Epoch: 0043 cost = 0.396512508\n",
      "Epoch: 0044 cost = 0.340283424\n",
      "Epoch: 0045 cost = 0.322572172\n",
      "Epoch: 0046 cost = 0.325184941\n",
      "Epoch: 0047 cost = 0.307041138\n",
      "Epoch: 0048 cost = 0.291416734\n",
      "Epoch: 0049 cost = 0.398760051\n",
      "Epoch: 0050 cost = 0.417851567\n",
      "Epoch: 0051 cost = 0.342076451\n",
      "Epoch: 0052 cost = 0.292048633\n",
      "Epoch: 0053 cost = 0.320230097\n",
      "Epoch: 0054 cost = 0.341484517\n",
      "Epoch: 0055 cost = 0.250225157\n",
      "Epoch: 0056 cost = 0.244798511\n",
      "Epoch: 0057 cost = 0.240461066\n",
      "Epoch: 0058 cost = 0.225971520\n",
      "Epoch: 0059 cost = 0.287144721\n",
      "Epoch: 0060 cost = 0.279480189\n",
      "Epoch: 0061 cost = 0.323952734\n",
      "Epoch: 0062 cost = 0.257749647\n",
      "Epoch: 0063 cost = 0.242425770\n",
      "Epoch: 0064 cost = 0.220576510\n",
      "Epoch: 0065 cost = 0.250183016\n",
      "Epoch: 0066 cost = 0.308028817\n",
      "Epoch: 0067 cost = 0.246220425\n",
      "Epoch: 0068 cost = 0.216464534\n",
      "Epoch: 0069 cost = 0.213959277\n",
      "Epoch: 0070 cost = 0.185831234\n",
      "Epoch: 0071 cost = 0.245312572\n",
      "Epoch: 0072 cost = 0.250117958\n",
      "Epoch: 0073 cost = 0.206567347\n",
      "Epoch: 0074 cost = 0.242626667\n",
      "Epoch: 0075 cost = 0.315547615\n",
      "Epoch: 0076 cost = 0.399150670\n",
      "Epoch: 0077 cost = 0.335664004\n",
      "Epoch: 0078 cost = 0.355732262\n",
      "Epoch: 0079 cost = 0.332475662\n",
      "Epoch: 0080 cost = 0.258419901\n",
      "Epoch: 0081 cost = 0.229744196\n",
      "Epoch: 0082 cost = 0.203376919\n",
      "Epoch: 0083 cost = 0.157348201\n",
      "Epoch: 0084 cost = 0.209071606\n",
      "Epoch: 0085 cost = 0.214949608\n",
      "Epoch: 0086 cost = 0.179042786\n",
      "Epoch: 0087 cost = 0.205866456\n",
      "Epoch: 0088 cost = 0.245669723\n",
      "Epoch: 0089 cost = 0.129701138\n",
      "Epoch: 0090 cost = 0.153034255\n",
      "Epoch: 0091 cost = 0.186804384\n",
      "Epoch: 0092 cost = 0.160652205\n",
      "Epoch: 0093 cost = 0.178063184\n",
      "Epoch: 0094 cost = 0.178010792\n",
      "Epoch: 0095 cost = 0.218883380\n",
      "Epoch: 0096 cost = 0.234302834\n",
      "Epoch: 0097 cost = 0.217731759\n",
      "Epoch: 0098 cost = 0.181152582\n",
      "Epoch: 0099 cost = 0.174209014\n",
      "Epoch: 0100 cost = 0.126019970\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(train_loader)\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hypothesis = model(images)\n",
    "\n",
    "        cost = criterion(hypothesis, labels)\n",
    "        cost.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"cost =\", \"{:.9f}\".format(avg_cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 82.14%\n",
      "Label: 0\n",
      "Prediction: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGfCAYAAAAZGgYhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYuUlEQVR4nO3df2hV9/3H8ddNk1xbzb0xtr13wWQLVMyKaGla9eJgTLOKlKE1hQ4Kk05WqldRM9gaWCuDQaRCay2JLd1mGcxlZDMtFtpOYr0yFp1elVrbhA1kBuK9Wf/IvWlmfpB8vn/0u0vvTG6Sm2veN8nzAQeac+69fvrJjycn93NyPM45JwAAZlmB9QAAAAsTAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJgovFsv3NTUpMOHDysWi2nNmjV64403tHbt2kmfNzY2pp6eHpWUlMjj8dyt4QEA7hLnnPr7+1VeXq6CggznOe4uaGlpccXFxe63v/2tu379uvvJT37iSktLXTwen/S53d3dThIbGxsb2xzfuru7M/68vysBWrt2rQuHw6mPR0dHXXl5uWtsbJz0uX19feaTxsbGxsY2862vry/jz/ucvwc0PDysaDSq2tra1L6CggLV1taqo6PjjscPDQ0pmUymtv7+/lwPCQBgYLK3UXIeoC+++EKjo6MKBAJp+wOBgGKx2B2Pb2xslN/vT20VFRW5HhIAIA+Zr4JraGhQIpFIbd3d3dZDAgDMgpyvgrv//vt1zz33KB6Pp+2Px+MKBoN3PN7r9crr9eZ6GACAPJfzM6Di4mLV1NSovb09tW9sbEzt7e0KhUK5/ucAAHPUXbkOqL6+Xjt27NBjjz2mtWvX6siRIxoYGNBzzz13N/45AMAcdFcC9Mwzz+jf//63Xn75ZcViMT3yyCP68MMP71iYAABYuDzOOWc9iK9LJpPy+/3WwwAAzFAikZDP55vwuPkqOADAwkSAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwUWg9ACAXmpqarIcwJ4TDYeshACmcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY8DjnnPUgvi6ZTMrv91sPA1nKtBx69+7dszgSzKbm5uYJj7H0e+FKJBLy+XwTHucMCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOA6INyhs7Mz4/GVK1dm9bp1dXUZj588eTKr14W9mfwY8Xg8ORwJ8gnXAQEA8hIBAgCYIEAAABMECABgggABAEwQIACAicLpPuHcuXM6fPiwotGobt26pba2Nm3bti113DmngwcP6u2331ZfX582bNigY8eOacWKFbkcN2aIZbPIpUxfE5Mt68/0tcjX2vw27TOggYEBrVmzZsL7vrzyyis6evSo3nzzTV24cEGLFy/W5s2bNTg4OOPBAgDmj2mfAW3ZskVbtmwZ95hzTkeOHNEvfvELbd26VZL0u9/9ToFAQO+++65++MMfzmy0AIB5I6fvAd24cUOxWEy1tbWpfX6/X+vWrVNHR8e4zxkaGlIymUzbAADzX04DFIvFJEmBQCBtfyAQSB37X42NjfL7/amtoqIil0MCAOQp81VwDQ0NSiQSqa27u9t6SACAWZDTAAWDQUlSPB5P2x+Px1PH/pfX65XP50vbAADz37QXIWRSVVWlYDCo9vZ2PfLII5K++uvWFy5c0K5du3L5T2ESky19zYSlr8il6urqjMe3b98+4TGWaM9v0w7Ql19+qX/+85+pj2/cuKGrV6+qrKxMlZWV2r9/v371q19pxYoVqqqq0ksvvaTy8vK0a4UAAJh2gC5duqTvfe97qY/r6+slSTt27NA777yjn/3sZxoYGNDzzz+vvr4+fec739GHH36oRYsW5W7UAIA5jxvSzVMzuakcv9rAbMr0K7g///nPEx7j6zT/cUM6AEBeIkAAABMECABgggABAEzk9Dog5I9Miwwk3sBF/jh58uSEx+rq6iY8Ntn6Kb7G8x9nQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmWIY9hzU1NVkPAbirMi3Rngy3csh/nAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATXAc0h+3evXvCY83NzbM4EmD2TXYtz2S3a4A9zoAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHhcnq1VTCaT8vv91sOYE/hz88DEMn1/dHV1ZXxudXV1roezICUSCfl8vgmPcwYEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIK/hg1gXsr0F+Ez/SV5zB7OgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAluxzCHcTsGIDuT/djj+yc3uB0DACAvESAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMDEtALU2Nioxx9/XCUlJXrwwQe1bds2dXV1pT1mcHBQ4XBYy5Yt05IlS1RXV6d4PJ7TQQMA5r5pBSgSiSgcDuv8+fM6ffq0RkZG9MQTT2hgYCD1mAMHDujUqVNqbW1VJBJRT0+Ptm/fnvOBAwDmODcDvb29TpKLRCLOOef6+vpcUVGRa21tTT3m888/d5JcR0fHlF4zkUg4SWxT2DKxHhsbWz5vk7Ee33zZEolExnme0XtAiURCklRWViZJikajGhkZUW1tbeox1dXVqqysVEdHx7ivMTQ0pGQymbYBAOa/rAM0Njam/fv3a8OGDVq1apUkKRaLqbi4WKWlpWmPDQQCisVi475OY2Oj/H5/aquoqMh2SACAOSTrAIXDYX366adqaWmZ0QAaGhqUSCRSW3d394xeDwAwNxRm86Q9e/bo/fff17lz57R8+fLU/mAwqOHhYfX19aWdBcXjcQWDwXFfy+v1yuv1ZjMMAMAcNq0zIOec9uzZo7a2Np05c0ZVVVVpx2tqalRUVKT29vbUvq6uLt28eVOhUCg3IwYAzAvTOgMKh8M6ceKE3nvvPZWUlKTe1/H7/br33nvl9/u1c+dO1dfXq6ysTD6fT3v37lUoFNL69evvyv8AAGCOmtLa6EmWJh4/fjz1mNu3b7vdu3e7pUuXuvvuu8899dRT7tatW1P+N1iGPfUtm88VGxsby7Bna5tsGbbn/yc7bySTSfn9futhzAmZPnUej2cWRwLMLZP92OP7JzcSiYR8Pt+Ex/lbcAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCi0HoAuDuampoyHg+Hw7M0EgAYH2dAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmPM45Zz2Ir0smk/L7/dbDmBM6OzsnPLZy5cqMz/V4PLkeDjBnTPZjr7m5ecJj3Mpk6hKJhHw+34THOQMCAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMFFoPQBkr7q6esJjeba6HgDuwBkQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAICJaQXo2LFjWr16tXw+n3w+n0KhkD744IPU8cHBQYXDYS1btkxLlixRXV2d4vF4zgcNAJj7phWg5cuX69ChQ4pGo7p06ZI2btyorVu36vr165KkAwcO6NSpU2ptbVUkElFPT4+2b99+VwYOAJjbPG6Gdy4rKyvT4cOH9fTTT+uBBx7QiRMn9PTTT0uSOjs79e1vf1sdHR1av379lF4vmUzK7/fPZEjQ5Dek83g8szQSIP9M9v3R3Nw84bFwOJzr4cxbiURCPp9vwuNZvwc0OjqqlpYWDQwMKBQKKRqNamRkRLW1tanHVFdXq7KyUh0dHRO+ztDQkJLJZNoGAJj/ph2ga9euacmSJfJ6vXrhhRfU1tamhx9+WLFYTMXFxSotLU17fCAQUCwWm/D1Ghsb5ff7U1tFRcW0/ycAAHPPtAO0cuVKXb16VRcuXNCuXbu0Y8cOffbZZ1kPoKGhQYlEIrV1d3dn/VoAgLmjcLpPKC4u1kMPPSRJqqmp0cWLF/X666/rmWee0fDwsPr6+tLOguLxuILB4ISv5/V65fV6pz9yAMCcNu0A/a+xsTENDQ2ppqZGRUVFam9vV11dnSSpq6tLN2/eVCgUmvFAkVtNTU0THuNNVix0fA/MjmkFqKGhQVu2bFFlZaX6+/t14sQJnT17Vh999JH8fr927typ+vp6lZWVyefzae/evQqFQlNeAQcAWDimFaDe3l796Ec/0q1bt+T3+7V69Wp99NFH+v73vy9Jeu2111RQUKC6ujoNDQ1p8+bNGZczAgAWrhlfB5RrXAeUG1znAEyM6+Rmx127DggAgJkgQAAAEwQIAGCCAAEATLAIYYHK9GnnDVjMdyxCmB0sQgAA5CUCBAAwQYAAACYIEADABAECAJggQAAAEzO+HQPmn87OzozHq6urZ2kkQPYy3XIE+YEzIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYILbMeAOk31JdHV1TXiMa4SQL7jliD1uxwAAyEsECABgggABAEwQIACACQIEADBBgAAAJrgdA+4w2RLVTMtbWfqK2ZJnV5AgC5wBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJhgGTamLdNy6myXaEv8lW3kFsv+8x9nQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwATXASGnZnLtBdcQ4X9l+rzX1dXN4khwN3AGBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCZdjIGxZLuCdbynvy5Mmsx4SvNDU1ZTy+e/fuCY9l+vzwuZn7OAMCAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmuA4I80K21xBxm4epmWyeZmIm139hbuMMCABgggABAEwQIACACQIEADBBgAAAJggQAMDEjJZhHzp0SA0NDdq3b5+OHDkiSRocHNRPf/pTtbS0aGhoSJs3b1Zzc7MCgUAuxgvk1GRLgLO9zcNUXjvb17WQaTm6tLCWpCN3sj4Dunjxot566y2tXr06bf+BAwd06tQptba2KhKJqKenR9u3b5/xQAEA80tWAfryyy/17LPP6u2339bSpUtT+xOJhH7zm9/o1Vdf1caNG1VTU6Pjx4/rb3/7m86fP5+zQQMA5r6sAhQOh/Xkk0+qtrY2bX80GtXIyEja/urqalVWVqqjo2Pc1xoaGlIymUzbAADz37TfA2ppadHly5d18eLFO47FYjEVFxertLQ0bX8gEFAsFhv39RobG/XLX/5yusMAAMxx0zoD6u7u1r59+/T73/9eixYtyskAGhoalEgkUlt3d3dOXhcAkN+mFaBoNKre3l49+uijKiwsVGFhoSKRiI4eParCwkIFAgENDw+rr68v7XnxeFzBYHDc1/R6vfL5fGkbAGD+m9av4DZt2qRr166l7XvuuedUXV2tn//856qoqFBRUZHa29tVV1cn6avlmzdv3lQoFMrdqIFZkmkpdWdnZ8bnZrucurm5OePxcDic1esC+WZaASopKdGqVavS9i1evFjLli1L7d+5c6fq6+tVVlYmn8+nvXv3KhQKaf369bkbNQBgzsv5/YBee+01FRQUqK6uLu1CVAAAvm7GATp79mzax4sWLVJTU5Oamppm+tIAgHmMvwUHADBBgAAAJggQAMAEAQIAmMj5KjhgoeAWBMDMcAYEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwkXcBcs5ZDwEAkAOT/TzPuwD19/dbDwEAkAOT/Tz3uDw75RgbG1NPT49KSkrk8XiUTCZVUVGh7u5u+Xw+6+HlLeZpapinqWGepoZ5Gp9zTv39/SovL1dBwcTnOYWzOKYpKSgo0PLly+/Y7/P5+ARPAfM0NczT1DBPU8M83cnv90/6mLz7FRwAYGEgQAAAE3kfIK/Xq4MHD8rr9VoPJa8xT1PDPE0N8zQ1zNPM5N0iBADAwpD3Z0AAgPmJAAEATBAgAIAJAgQAMJH3AWpqatK3vvUtLVq0SOvWrdPf//536yGZOnfunH7wgx+ovLxcHo9H7777btpx55xefvllfeMb39C9996r2tpa/eMf/7AZrJHGxkY9/vjjKikp0YMPPqht27apq6sr7TGDg4MKh8NatmyZlixZorq6OsXjcaMR2zh27JhWr16duogyFArpgw8+SB1njsZ36NAheTwe7d+/P7WPucpOXgfoj3/8o+rr63Xw4EFdvnxZa9as0ebNm9Xb22s9NDMDAwNas2aNmpqaxj3+yiuv6OjRo3rzzTd14cIFLV68WJs3b9bg4OAsj9ROJBJROBzW+fPndfr0aY2MjOiJJ57QwMBA6jEHDhzQqVOn1Nraqkgkop6eHm3fvt1w1LNv+fLlOnTokKLRqC5duqSNGzdq69atun79uiTmaDwXL17UW2+9pdWrV6ftZ66y5PLY2rVrXTgcTn08OjrqysvLXWNjo+Go8ock19bWlvp4bGzMBYNBd/jw4dS+vr4+5/V63R/+8AeDEeaH3t5eJ8lFIhHn3FdzUlRU5FpbW1OP+fzzz50k19HRYTXMvLB06VL361//mjkaR39/v1uxYoU7ffq0++53v+v27dvnnOPraSby9gxoeHhY0WhUtbW1qX0FBQWqra1VR0eH4cjy140bNxSLxdLmzO/3a926dQt6zhKJhCSprKxMkhSNRjUyMpI2T9XV1aqsrFyw8zQ6OqqWlhYNDAwoFAoxR+MIh8N68skn0+ZE4utpJvLuj5H+1xdffKHR0VEFAoG0/YFAQJ2dnUajym+xWEySxp2z/x5baMbGxrR//35t2LBBq1atkvTVPBUXF6u0tDTtsQtxnq5du6ZQKKTBwUEtWbJEbW1tevjhh3X16lXm6GtaWlp0+fJlXbx48Y5jfD1lL28DBORCOBzWp59+qr/+9a/WQ8lLK1eu1NWrV5VIJPSnP/1JO3bsUCQSsR5WXunu7ta+fft0+vRpLVq0yHo480re/gru/vvv1z333HPHSpJ4PK5gMGg0qvz233lhzr6yZ88evf/++/r444/TbvERDAY1PDysvr6+tMcvxHkqLi7WQw89pJqaGjU2NmrNmjV6/fXXmaOviUaj6u3t1aOPPqrCwkIVFhYqEono6NGjKiwsVCAQYK6ylLcBKi4uVk1Njdrb21P7xsbG1N7erlAoZDiy/FVVVaVgMJg2Z8lkUhcuXFhQc+ac0549e9TW1qYzZ86oqqoq7XhNTY2KiorS5qmrq0s3b95cUPM0nrGxMQ0NDTFHX7Np0yZdu3ZNV69eTW2PPfaYnn322dR/M1dZsl4FkUlLS4vzer3unXfecZ999pl7/vnnXWlpqYvFYtZDM9Pf3++uXLnirly54iS5V1991V25csX961//cs45d+jQIVdaWuree+8998knn7itW7e6qqoqd/v2beORz55du3Y5v9/vzp49627dupXa/vOf/6Qe88ILL7jKykp35swZd+nSJRcKhVwoFDIc9ex78cUXXSQScTdu3HCffPKJe/HFF53H43F/+ctfnHPMUSZfXwXnHHOVrbwOkHPOvfHGG66ystIVFxe7tWvXuvPnz1sPydTHH3/sJN2x7dixwzn31VLsl156yQUCAef1et2mTZtcV1eX7aBn2XjzI8kdP3489Zjbt2+73bt3u6VLl7r77rvPPfXUU+7WrVt2gzbw4x//2H3zm990xcXF7oEHHnCbNm1Kxcc55iiT/w0Qc5UdbscAADCRt+8BAQDmNwIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAxP8BdEBrztHaFA0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 테스트 데이터를 사용하여 모델을 평가\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # torch.no_grad()를 하면 gradient 계산을 수행하지 않음\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')\n",
    "\n",
    "# 테스트 데이터에서 무작위로 하나를 뽑아서 예측\n",
    "r = random.randint(0, len(test_dataset) - 1)\n",
    "single_image, single_label = test_dataset[r]\n",
    "single_image = single_image.unsqueeze(0)  # 배치 차원을 추가\n",
    "with torch.no_grad():\n",
    "    single_prediction = model(single_image)\n",
    "    predicted_label = torch.argmax(single_prediction, 1).item()\n",
    "\n",
    "print(f'Label: {single_label.item()}')\n",
    "print(f'Prediction: {predicted_label}')\n",
    "\n",
    "# 이미지 출력\n",
    "plt.imshow(single_image.squeeze().numpy(), cmap=\"Greys\", interpolation=\"nearest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 배열 크기 : 280\n",
      "첫번째 원소값 사이즈: torch.Size([1, 50, 50])\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch [1/10], Loss: 0.6920\n",
      "Epoch [2/10], Loss: 0.6891\n",
      "Epoch [3/10], Loss: 0.6866\n",
      "Epoch [4/10], Loss: 0.6844\n",
      "Epoch [5/10], Loss: 0.6807\n",
      "Epoch [6/10], Loss: 0.6784\n",
      "Epoch [7/10], Loss: 0.6749\n",
      "Epoch [8/10], Loss: 0.6722\n",
      "Epoch [9/10], Loss: 0.6683\n",
      "Epoch [10/10], Loss: 0.6646\n",
      "Epoch [11/10], Loss: 0.6602\n",
      "Epoch [12/10], Loss: 0.6558\n",
      "Epoch [13/10], Loss: 0.6506\n",
      "Epoch [14/10], Loss: 0.6450\n",
      "Epoch [15/10], Loss: 0.6393\n",
      "Epoch [16/10], Loss: 0.6327\n",
      "Epoch [17/10], Loss: 0.6255\n",
      "Epoch [18/10], Loss: 0.6180\n",
      "Epoch [19/10], Loss: 0.6100\n",
      "Epoch [20/10], Loss: 0.6021\n",
      "Epoch [21/10], Loss: 0.5944\n",
      "Epoch [22/10], Loss: 0.5872\n",
      "Epoch [23/10], Loss: 0.5788\n",
      "Epoch [24/10], Loss: 0.5692\n",
      "Epoch [25/10], Loss: 0.5607\n",
      "Epoch [26/10], Loss: 0.5540\n",
      "Epoch [27/10], Loss: 0.5479\n",
      "Epoch [28/10], Loss: 0.5395\n",
      "Epoch [29/10], Loss: 0.5283\n",
      "Epoch [30/10], Loss: 0.5211\n",
      "Epoch [31/10], Loss: 0.5170\n",
      "Epoch [32/10], Loss: 0.5107\n",
      "Epoch [33/10], Loss: 0.4996\n",
      "Epoch [34/10], Loss: 0.4907\n",
      "Epoch [35/10], Loss: 0.4867\n",
      "Epoch [36/10], Loss: 0.4831\n",
      "Epoch [37/10], Loss: 0.4742\n",
      "Epoch [38/10], Loss: 0.4630\n",
      "Epoch [39/10], Loss: 0.4567\n",
      "Epoch [40/10], Loss: 0.4547\n",
      "Epoch [41/10], Loss: 0.4518\n",
      "Epoch [42/10], Loss: 0.4422\n",
      "Epoch [43/10], Loss: 0.4307\n",
      "Epoch [44/10], Loss: 0.4257\n",
      "Epoch [45/10], Loss: 0.4254\n",
      "Epoch [46/10], Loss: 0.4233\n",
      "Epoch [47/10], Loss: 0.4133\n",
      "Epoch [48/10], Loss: 0.4019\n",
      "Epoch [49/10], Loss: 0.3976\n",
      "Epoch [50/10], Loss: 0.3982\n",
      "Epoch [51/10], Loss: 0.3973\n",
      "Epoch [52/10], Loss: 0.3884\n",
      "Epoch [53/10], Loss: 0.3766\n",
      "Epoch [54/10], Loss: 0.3706\n",
      "Epoch [55/10], Loss: 0.3710\n",
      "Epoch [56/10], Loss: 0.3728\n",
      "Epoch [57/10], Loss: 0.3688\n",
      "Epoch [58/10], Loss: 0.3578\n",
      "Epoch [59/10], Loss: 0.3472\n",
      "Epoch [60/10], Loss: 0.3444\n",
      "Epoch [61/10], Loss: 0.3469\n",
      "Epoch [62/10], Loss: 0.3481\n",
      "Epoch [63/10], Loss: 0.3425\n",
      "Epoch [64/10], Loss: 0.3301\n",
      "Epoch [65/10], Loss: 0.3218\n",
      "Epoch [66/10], Loss: 0.3216\n",
      "Epoch [67/10], Loss: 0.3261\n",
      "Epoch [68/10], Loss: 0.3290\n",
      "Epoch [69/10], Loss: 0.3205\n",
      "Epoch [70/10], Loss: 0.3064\n",
      "Epoch [71/10], Loss: 0.2996\n",
      "Epoch [72/10], Loss: 0.3027\n",
      "Epoch [73/10], Loss: 0.3076\n",
      "Epoch [74/10], Loss: 0.3035\n",
      "Epoch [75/10], Loss: 0.2919\n",
      "Epoch [76/10], Loss: 0.2825\n",
      "Epoch [77/10], Loss: 0.2822\n",
      "Epoch [78/10], Loss: 0.2866\n",
      "Epoch [79/10], Loss: 0.2874\n",
      "Epoch [80/10], Loss: 0.2813\n",
      "Epoch [81/10], Loss: 0.2701\n",
      "Epoch [82/10], Loss: 0.2633\n",
      "Epoch [83/10], Loss: 0.2632\n",
      "Epoch [84/10], Loss: 0.2663\n",
      "Epoch [85/10], Loss: 0.2691\n",
      "Epoch [86/10], Loss: 0.2657\n",
      "Epoch [87/10], Loss: 0.2577\n",
      "Epoch [88/10], Loss: 0.2479\n",
      "Epoch [89/10], Loss: 0.2427\n",
      "Epoch [90/10], Loss: 0.2428\n",
      "Epoch [91/10], Loss: 0.2457\n",
      "Epoch [92/10], Loss: 0.2499\n",
      "Epoch [93/10], Loss: 0.2501\n",
      "Epoch [94/10], Loss: 0.2460\n",
      "Epoch [95/10], Loss: 0.2345\n",
      "Epoch [96/10], Loss: 0.2251\n",
      "Epoch [97/10], Loss: 0.2219\n",
      "Epoch [98/10], Loss: 0.2242\n",
      "Epoch [99/10], Loss: 0.2291\n",
      "Epoch [100/10], Loss: 0.2312\n",
      "Accuracy of the model on fold 1: 76.79%\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Epoch [1/10], Loss: 0.7017\n",
      "Epoch [2/10], Loss: 0.6922\n",
      "Epoch [3/10], Loss: 0.6910\n",
      "Epoch [4/10], Loss: 0.6898\n",
      "Epoch [5/10], Loss: 0.6878\n",
      "Epoch [6/10], Loss: 0.6864\n",
      "Epoch [7/10], Loss: 0.6845\n",
      "Epoch [8/10], Loss: 0.6823\n",
      "Epoch [9/10], Loss: 0.6799\n",
      "Epoch [10/10], Loss: 0.6775\n",
      "Epoch [11/10], Loss: 0.6748\n",
      "Epoch [12/10], Loss: 0.6720\n",
      "Epoch [13/10], Loss: 0.6682\n",
      "Epoch [14/10], Loss: 0.6648\n",
      "Epoch [15/10], Loss: 0.6607\n",
      "Epoch [16/10], Loss: 0.6565\n",
      "Epoch [17/10], Loss: 0.6520\n",
      "Epoch [18/10], Loss: 0.6468\n",
      "Epoch [19/10], Loss: 0.6413\n",
      "Epoch [20/10], Loss: 0.6353\n",
      "Epoch [21/10], Loss: 0.6285\n",
      "Epoch [22/10], Loss: 0.6215\n",
      "Epoch [23/10], Loss: 0.6139\n",
      "Epoch [24/10], Loss: 0.6054\n",
      "Epoch [25/10], Loss: 0.5971\n",
      "Epoch [26/10], Loss: 0.5918\n",
      "Epoch [27/10], Loss: 0.5878\n",
      "Epoch [28/10], Loss: 0.5758\n",
      "Epoch [29/10], Loss: 0.5713\n",
      "Epoch [30/10], Loss: 0.5662\n",
      "Epoch [31/10], Loss: 0.5560\n",
      "Epoch [32/10], Loss: 0.5536\n",
      "Epoch [33/10], Loss: 0.5455\n",
      "Epoch [34/10], Loss: 0.5380\n",
      "Epoch [35/10], Loss: 0.5348\n",
      "Epoch [36/10], Loss: 0.5258\n",
      "Epoch [37/10], Loss: 0.5186\n",
      "Epoch [38/10], Loss: 0.5148\n",
      "Epoch [39/10], Loss: 0.5065\n",
      "Epoch [40/10], Loss: 0.4971\n",
      "Epoch [41/10], Loss: 0.4919\n",
      "Epoch [42/10], Loss: 0.4883\n",
      "Epoch [43/10], Loss: 0.4820\n",
      "Epoch [44/10], Loss: 0.4718\n",
      "Epoch [45/10], Loss: 0.4633\n",
      "Epoch [46/10], Loss: 0.4588\n",
      "Epoch [47/10], Loss: 0.4568\n",
      "Epoch [48/10], Loss: 0.4547\n",
      "Epoch [49/10], Loss: 0.4455\n",
      "Epoch [50/10], Loss: 0.4337\n",
      "Epoch [51/10], Loss: 0.4292\n",
      "Epoch [52/10], Loss: 0.4297\n",
      "Epoch [53/10], Loss: 0.4266\n",
      "Epoch [54/10], Loss: 0.4155\n",
      "Epoch [55/10], Loss: 0.4066\n",
      "Epoch [56/10], Loss: 0.4053\n",
      "Epoch [57/10], Loss: 0.4055\n",
      "Epoch [58/10], Loss: 0.4003\n",
      "Epoch [59/10], Loss: 0.3895\n",
      "Epoch [60/10], Loss: 0.3827\n",
      "Epoch [61/10], Loss: 0.3820\n",
      "Epoch [62/10], Loss: 0.3823\n",
      "Epoch [63/10], Loss: 0.3789\n",
      "Epoch [64/10], Loss: 0.3695\n",
      "Epoch [65/10], Loss: 0.3609\n",
      "Epoch [66/10], Loss: 0.3575\n",
      "Epoch [67/10], Loss: 0.3580\n",
      "Epoch [68/10], Loss: 0.3586\n",
      "Epoch [69/10], Loss: 0.3544\n",
      "Epoch [70/10], Loss: 0.3456\n",
      "Epoch [71/10], Loss: 0.3370\n",
      "Epoch [72/10], Loss: 0.3333\n",
      "Epoch [73/10], Loss: 0.3337\n",
      "Epoch [74/10], Loss: 0.3355\n",
      "Epoch [75/10], Loss: 0.3355\n",
      "Epoch [76/10], Loss: 0.3289\n",
      "Epoch [77/10], Loss: 0.3182\n",
      "Epoch [78/10], Loss: 0.3111\n",
      "Epoch [79/10], Loss: 0.3105\n",
      "Epoch [80/10], Loss: 0.3135\n",
      "Epoch [81/10], Loss: 0.3159\n",
      "Epoch [82/10], Loss: 0.3127\n",
      "Epoch [83/10], Loss: 0.3015\n",
      "Epoch [84/10], Loss: 0.2920\n",
      "Epoch [85/10], Loss: 0.2904\n",
      "Epoch [86/10], Loss: 0.2939\n",
      "Epoch [87/10], Loss: 0.2961\n",
      "Epoch [88/10], Loss: 0.2914\n",
      "Epoch [89/10], Loss: 0.2816\n",
      "Epoch [90/10], Loss: 0.2740\n",
      "Epoch [91/10], Loss: 0.2727\n",
      "Epoch [92/10], Loss: 0.2753\n",
      "Epoch [93/10], Loss: 0.2771\n",
      "Epoch [94/10], Loss: 0.2747\n",
      "Epoch [95/10], Loss: 0.2672\n",
      "Epoch [96/10], Loss: 0.2590\n",
      "Epoch [97/10], Loss: 0.2543\n",
      "Epoch [98/10], Loss: 0.2539\n",
      "Epoch [99/10], Loss: 0.2558\n",
      "Epoch [100/10], Loss: 0.2581\n",
      "Accuracy of the model on fold 2: 75.00%\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Epoch [1/10], Loss: 0.6936\n",
      "Epoch [2/10], Loss: 0.6950\n",
      "Epoch [3/10], Loss: 0.6885\n",
      "Epoch [4/10], Loss: 0.6872\n",
      "Epoch [5/10], Loss: 0.6842\n",
      "Epoch [6/10], Loss: 0.6827\n",
      "Epoch [7/10], Loss: 0.6793\n",
      "Epoch [8/10], Loss: 0.6776\n",
      "Epoch [9/10], Loss: 0.6742\n",
      "Epoch [10/10], Loss: 0.6710\n",
      "Epoch [11/10], Loss: 0.6673\n",
      "Epoch [12/10], Loss: 0.6628\n",
      "Epoch [13/10], Loss: 0.6587\n",
      "Epoch [14/10], Loss: 0.6544\n",
      "Epoch [15/10], Loss: 0.6493\n",
      "Epoch [16/10], Loss: 0.6442\n",
      "Epoch [17/10], Loss: 0.6374\n",
      "Epoch [18/10], Loss: 0.6318\n",
      "Epoch [19/10], Loss: 0.6251\n",
      "Epoch [20/10], Loss: 0.6178\n",
      "Epoch [21/10], Loss: 0.6107\n",
      "Epoch [22/10], Loss: 0.6030\n",
      "Epoch [23/10], Loss: 0.5947\n",
      "Epoch [24/10], Loss: 0.5859\n",
      "Epoch [25/10], Loss: 0.5768\n",
      "Epoch [26/10], Loss: 0.5671\n",
      "Epoch [27/10], Loss: 0.5571\n",
      "Epoch [28/10], Loss: 0.5468\n",
      "Epoch [29/10], Loss: 0.5383\n",
      "Epoch [30/10], Loss: 0.5476\n",
      "Epoch [31/10], Loss: 0.5456\n",
      "Epoch [32/10], Loss: 0.5159\n",
      "Epoch [33/10], Loss: 0.5351\n",
      "Epoch [34/10], Loss: 0.5048\n",
      "Epoch [35/10], Loss: 0.5228\n",
      "Epoch [36/10], Loss: 0.4953\n",
      "Epoch [37/10], Loss: 0.5108\n",
      "Epoch [38/10], Loss: 0.4864\n",
      "Epoch [39/10], Loss: 0.4998\n",
      "Epoch [40/10], Loss: 0.4777\n",
      "Epoch [41/10], Loss: 0.4892\n",
      "Epoch [42/10], Loss: 0.4690\n",
      "Epoch [43/10], Loss: 0.4791\n",
      "Epoch [44/10], Loss: 0.4601\n",
      "Epoch [45/10], Loss: 0.4689\n",
      "Epoch [46/10], Loss: 0.4518\n",
      "Epoch [47/10], Loss: 0.4582\n",
      "Epoch [48/10], Loss: 0.4442\n",
      "Epoch [49/10], Loss: 0.4465\n",
      "Epoch [50/10], Loss: 0.4379\n",
      "Epoch [51/10], Loss: 0.4338\n",
      "Epoch [52/10], Loss: 0.4325\n",
      "Epoch [53/10], Loss: 0.4219\n",
      "Epoch [54/10], Loss: 0.4244\n",
      "Epoch [55/10], Loss: 0.4145\n",
      "Epoch [56/10], Loss: 0.4113\n",
      "Epoch [57/10], Loss: 0.4103\n",
      "Epoch [58/10], Loss: 0.4005\n",
      "Epoch [59/10], Loss: 0.3989\n",
      "Epoch [60/10], Loss: 0.3967\n",
      "Epoch [61/10], Loss: 0.3879\n",
      "Epoch [62/10], Loss: 0.3842\n",
      "Epoch [63/10], Loss: 0.3836\n",
      "Epoch [64/10], Loss: 0.3780\n",
      "Epoch [65/10], Loss: 0.3703\n",
      "Epoch [66/10], Loss: 0.3662\n",
      "Epoch [67/10], Loss: 0.3649\n",
      "Epoch [68/10], Loss: 0.3631\n",
      "Epoch [69/10], Loss: 0.3593\n",
      "Epoch [70/10], Loss: 0.3533\n",
      "Epoch [71/10], Loss: 0.3468\n",
      "Epoch [72/10], Loss: 0.3409\n",
      "Epoch [73/10], Loss: 0.3359\n",
      "Epoch [74/10], Loss: 0.3317\n",
      "Epoch [75/10], Loss: 0.3281\n",
      "Epoch [76/10], Loss: 0.3259\n",
      "Epoch [77/10], Loss: 0.3302\n",
      "Epoch [78/10], Loss: 0.3582\n",
      "Epoch [79/10], Loss: 0.4076\n",
      "Epoch [80/10], Loss: 0.3581\n",
      "Epoch [81/10], Loss: 0.3096\n",
      "Epoch [82/10], Loss: 0.3684\n",
      "Epoch [83/10], Loss: 0.3231\n",
      "Epoch [84/10], Loss: 0.3142\n",
      "Epoch [85/10], Loss: 0.3453\n",
      "Epoch [86/10], Loss: 0.2953\n",
      "Epoch [87/10], Loss: 0.3245\n",
      "Epoch [88/10], Loss: 0.3091\n",
      "Epoch [89/10], Loss: 0.2935\n",
      "Epoch [90/10], Loss: 0.3157\n",
      "Epoch [91/10], Loss: 0.2843\n",
      "Epoch [92/10], Loss: 0.3000\n",
      "Epoch [93/10], Loss: 0.2932\n",
      "Epoch [94/10], Loss: 0.2788\n",
      "Epoch [95/10], Loss: 0.2951\n",
      "Epoch [96/10], Loss: 0.2756\n",
      "Epoch [97/10], Loss: 0.2781\n",
      "Epoch [98/10], Loss: 0.2817\n",
      "Epoch [99/10], Loss: 0.2657\n",
      "Epoch [100/10], Loss: 0.2740\n",
      "Accuracy of the model on fold 3: 78.57%\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Epoch [1/10], Loss: 0.6938\n",
      "Epoch [2/10], Loss: 0.6917\n",
      "Epoch [3/10], Loss: 0.6888\n",
      "Epoch [4/10], Loss: 0.6870\n",
      "Epoch [5/10], Loss: 0.6845\n",
      "Epoch [6/10], Loss: 0.6819\n",
      "Epoch [7/10], Loss: 0.6791\n",
      "Epoch [8/10], Loss: 0.6762\n",
      "Epoch [9/10], Loss: 0.6728\n",
      "Epoch [10/10], Loss: 0.6690\n",
      "Epoch [11/10], Loss: 0.6649\n",
      "Epoch [12/10], Loss: 0.6603\n",
      "Epoch [13/10], Loss: 0.6554\n",
      "Epoch [14/10], Loss: 0.6499\n",
      "Epoch [15/10], Loss: 0.6439\n",
      "Epoch [16/10], Loss: 0.6368\n",
      "Epoch [17/10], Loss: 0.6293\n",
      "Epoch [18/10], Loss: 0.6215\n",
      "Epoch [19/10], Loss: 0.6135\n",
      "Epoch [20/10], Loss: 0.6051\n",
      "Epoch [21/10], Loss: 0.5975\n",
      "Epoch [22/10], Loss: 0.5896\n",
      "Epoch [23/10], Loss: 0.5804\n",
      "Epoch [24/10], Loss: 0.5719\n",
      "Epoch [25/10], Loss: 0.5642\n",
      "Epoch [26/10], Loss: 0.5543\n",
      "Epoch [27/10], Loss: 0.5458\n",
      "Epoch [28/10], Loss: 0.5373\n",
      "Epoch [29/10], Loss: 0.5291\n",
      "Epoch [30/10], Loss: 0.5216\n",
      "Epoch [31/10], Loss: 0.5156\n",
      "Epoch [32/10], Loss: 0.5092\n",
      "Epoch [33/10], Loss: 0.4972\n",
      "Epoch [34/10], Loss: 0.4876\n",
      "Epoch [35/10], Loss: 0.4844\n",
      "Epoch [36/10], Loss: 0.4804\n",
      "Epoch [37/10], Loss: 0.4697\n",
      "Epoch [38/10], Loss: 0.4595\n",
      "Epoch [39/10], Loss: 0.4557\n",
      "Epoch [40/10], Loss: 0.4544\n",
      "Epoch [41/10], Loss: 0.4476\n",
      "Epoch [42/10], Loss: 0.4350\n",
      "Epoch [43/10], Loss: 0.4281\n",
      "Epoch [44/10], Loss: 0.4276\n",
      "Epoch [45/10], Loss: 0.4251\n",
      "Epoch [46/10], Loss: 0.4155\n",
      "Epoch [47/10], Loss: 0.4045\n",
      "Epoch [48/10], Loss: 0.4007\n",
      "Epoch [49/10], Loss: 0.4014\n",
      "Epoch [50/10], Loss: 0.4000\n",
      "Epoch [51/10], Loss: 0.3912\n",
      "Epoch [52/10], Loss: 0.3791\n",
      "Epoch [53/10], Loss: 0.3746\n",
      "Epoch [54/10], Loss: 0.3766\n",
      "Epoch [55/10], Loss: 0.3778\n",
      "Epoch [56/10], Loss: 0.3702\n",
      "Epoch [57/10], Loss: 0.3566\n",
      "Epoch [58/10], Loss: 0.3517\n",
      "Epoch [59/10], Loss: 0.3549\n",
      "Epoch [60/10], Loss: 0.3549\n",
      "Epoch [61/10], Loss: 0.3459\n",
      "Epoch [62/10], Loss: 0.3347\n",
      "Epoch [63/10], Loss: 0.3314\n",
      "Epoch [64/10], Loss: 0.3341\n",
      "Epoch [65/10], Loss: 0.3348\n",
      "Epoch [66/10], Loss: 0.3285\n",
      "Epoch [67/10], Loss: 0.3173\n",
      "Epoch [68/10], Loss: 0.3108\n",
      "Epoch [69/10], Loss: 0.3110\n",
      "Epoch [70/10], Loss: 0.3133\n",
      "Epoch [71/10], Loss: 0.3135\n",
      "Epoch [72/10], Loss: 0.3068\n",
      "Epoch [73/10], Loss: 0.2971\n",
      "Epoch [74/10], Loss: 0.2896\n",
      "Epoch [75/10], Loss: 0.2876\n",
      "Epoch [76/10], Loss: 0.2894\n",
      "Epoch [77/10], Loss: 0.2918\n",
      "Epoch [78/10], Loss: 0.2924\n",
      "Epoch [79/10], Loss: 0.2853\n",
      "Epoch [80/10], Loss: 0.2750\n",
      "Epoch [81/10], Loss: 0.2665\n",
      "Epoch [82/10], Loss: 0.2638\n",
      "Epoch [83/10], Loss: 0.2657\n",
      "Epoch [84/10], Loss: 0.2699\n",
      "Epoch [85/10], Loss: 0.2748\n",
      "Epoch [86/10], Loss: 0.2710\n",
      "Epoch [87/10], Loss: 0.2602\n",
      "Epoch [88/10], Loss: 0.2472\n",
      "Epoch [89/10], Loss: 0.2436\n",
      "Epoch [90/10], Loss: 0.2482\n",
      "Epoch [91/10], Loss: 0.2530\n",
      "Epoch [92/10], Loss: 0.2522\n",
      "Epoch [93/10], Loss: 0.2409\n",
      "Epoch [94/10], Loss: 0.2304\n",
      "Epoch [95/10], Loss: 0.2276\n",
      "Epoch [96/10], Loss: 0.2310\n",
      "Epoch [97/10], Loss: 0.2350\n",
      "Epoch [98/10], Loss: 0.2327\n",
      "Epoch [99/10], Loss: 0.2265\n",
      "Epoch [100/10], Loss: 0.2173\n",
      "Accuracy of the model on fold 4: 80.36%\n",
      "FOLD 5\n",
      "--------------------------------\n",
      "Epoch [1/10], Loss: 0.6933\n",
      "Epoch [2/10], Loss: 0.6912\n",
      "Epoch [3/10], Loss: 0.6876\n",
      "Epoch [4/10], Loss: 0.6848\n",
      "Epoch [5/10], Loss: 0.6818\n",
      "Epoch [6/10], Loss: 0.6783\n",
      "Epoch [7/10], Loss: 0.6745\n",
      "Epoch [8/10], Loss: 0.6703\n",
      "Epoch [9/10], Loss: 0.6654\n",
      "Epoch [10/10], Loss: 0.6600\n",
      "Epoch [11/10], Loss: 0.6538\n",
      "Epoch [12/10], Loss: 0.6474\n",
      "Epoch [13/10], Loss: 0.6399\n",
      "Epoch [14/10], Loss: 0.6322\n",
      "Epoch [15/10], Loss: 0.6257\n",
      "Epoch [16/10], Loss: 0.6171\n",
      "Epoch [17/10], Loss: 0.6103\n",
      "Epoch [18/10], Loss: 0.6035\n",
      "Epoch [19/10], Loss: 0.5946\n",
      "Epoch [20/10], Loss: 0.5867\n",
      "Epoch [21/10], Loss: 0.5799\n",
      "Epoch [22/10], Loss: 0.5718\n",
      "Epoch [23/10], Loss: 0.5622\n",
      "Epoch [24/10], Loss: 0.5535\n",
      "Epoch [25/10], Loss: 0.5459\n",
      "Epoch [26/10], Loss: 0.5395\n",
      "Epoch [27/10], Loss: 0.5323\n",
      "Epoch [28/10], Loss: 0.5214\n",
      "Epoch [29/10], Loss: 0.5113\n",
      "Epoch [30/10], Loss: 0.5064\n",
      "Epoch [31/10], Loss: 0.5022\n",
      "Epoch [32/10], Loss: 0.4920\n",
      "Epoch [33/10], Loss: 0.4812\n",
      "Epoch [34/10], Loss: 0.4761\n",
      "Epoch [35/10], Loss: 0.4729\n",
      "Epoch [36/10], Loss: 0.4653\n",
      "Epoch [37/10], Loss: 0.4536\n",
      "Epoch [38/10], Loss: 0.4463\n",
      "Epoch [39/10], Loss: 0.4437\n",
      "Epoch [40/10], Loss: 0.4409\n",
      "Epoch [41/10], Loss: 0.4327\n",
      "Epoch [42/10], Loss: 0.4210\n",
      "Epoch [43/10], Loss: 0.4145\n",
      "Epoch [44/10], Loss: 0.4135\n",
      "Epoch [45/10], Loss: 0.4119\n",
      "Epoch [46/10], Loss: 0.4043\n",
      "Epoch [47/10], Loss: 0.3924\n",
      "Epoch [48/10], Loss: 0.3860\n",
      "Epoch [49/10], Loss: 0.3858\n",
      "Epoch [50/10], Loss: 0.3860\n",
      "Epoch [51/10], Loss: 0.3808\n",
      "Epoch [52/10], Loss: 0.3685\n",
      "Epoch [53/10], Loss: 0.3600\n",
      "Epoch [54/10], Loss: 0.3592\n",
      "Epoch [55/10], Loss: 0.3611\n",
      "Epoch [56/10], Loss: 0.3593\n",
      "Epoch [57/10], Loss: 0.3488\n",
      "Epoch [58/10], Loss: 0.3381\n",
      "Epoch [59/10], Loss: 0.3346\n",
      "Epoch [60/10], Loss: 0.3366\n",
      "Epoch [61/10], Loss: 0.3387\n",
      "Epoch [62/10], Loss: 0.3331\n",
      "Epoch [63/10], Loss: 0.3219\n",
      "Epoch [64/10], Loss: 0.3133\n",
      "Epoch [65/10], Loss: 0.3122\n",
      "Epoch [66/10], Loss: 0.3152\n",
      "Epoch [67/10], Loss: 0.3156\n",
      "Epoch [68/10], Loss: 0.3099\n",
      "Epoch [69/10], Loss: 0.2991\n",
      "Epoch [70/10], Loss: 0.2918\n",
      "Epoch [71/10], Loss: 0.2904\n",
      "Epoch [72/10], Loss: 0.2927\n",
      "Epoch [73/10], Loss: 0.2950\n",
      "Epoch [74/10], Loss: 0.2923\n",
      "Epoch [75/10], Loss: 0.2843\n",
      "Epoch [76/10], Loss: 0.2744\n",
      "Epoch [77/10], Loss: 0.2692\n",
      "Epoch [78/10], Loss: 0.2692\n",
      "Epoch [79/10], Loss: 0.2718\n",
      "Epoch [80/10], Loss: 0.2743\n",
      "Epoch [81/10], Loss: 0.2724\n",
      "Epoch [82/10], Loss: 0.2653\n",
      "Epoch [83/10], Loss: 0.2553\n",
      "Epoch [84/10], Loss: 0.2488\n",
      "Epoch [85/10], Loss: 0.2477\n",
      "Epoch [86/10], Loss: 0.2501\n",
      "Epoch [87/10], Loss: 0.2535\n",
      "Epoch [88/10], Loss: 0.2542\n",
      "Epoch [89/10], Loss: 0.2501\n",
      "Epoch [90/10], Loss: 0.2405\n",
      "Epoch [91/10], Loss: 0.2317\n",
      "Epoch [92/10], Loss: 0.2280\n",
      "Epoch [93/10], Loss: 0.2294\n",
      "Epoch [94/10], Loss: 0.2331\n",
      "Epoch [95/10], Loss: 0.2364\n",
      "Epoch [96/10], Loss: 0.2360\n",
      "Epoch [97/10], Loss: 0.2289\n",
      "Epoch [98/10], Loss: 0.2185\n",
      "Epoch [99/10], Loss: 0.2115\n",
      "Epoch [100/10], Loss: 0.2107\n",
      "Accuracy of the model on fold 5: 62.50%\n",
      "--------------------------------\n",
      "K-FOLD CROSS VALIDATION RESULTS\n",
      "--------------------------------\n",
      "All fold results: [76.78571428571429, 75.0, 78.57142857142857, 80.35714285714286, 62.5]\n",
      "Average accuracy: 74.64%\n"
     ]
    }
   ],
   "source": [
    "###여기서부터는 챗 gpt 코드. \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "image_list1 = [os.path.join(path, fname) for fname in os.listdir(path) if fname.endswith('.png')]\n",
    "image_list2 = [os.path.join(path, fname) for fname in os.listdir(path) if fname.endswith('.jpg')]\n",
    "\n",
    "image_list = image_list1 + image_list2\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# 이미지 데이터셋과 레이블 리스트 초기화\n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "for idx in image_list:\n",
    "    # 이미지 읽기 및 전처리\n",
    "    image = cv2.imread(idx, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (50, 50))\n",
    "    image = Image.fromarray(image)\n",
    "    image = transform(image)\n",
    "    dataset.append(image)\n",
    "    \n",
    "    # 파일 이름에서 레이블 추출 ('o' 또는 'x'가 파일 이름에 포함되어 있는 경우)\n",
    "    filename = os.path.basename(idx).lower()\n",
    "    if 'o' in filename:\n",
    "        labels.append(0)  # 결과 레이블 'o'\n",
    "    elif 'x' in filename:\n",
    "        labels.append(1)  # 결과 레이블 'x'\n",
    "\n",
    "print(f\"dataset 배열 크기 : {len(dataset)}\")\n",
    "print(f\"첫번째 원소값 사이즈: {dataset[0].size()}\")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 리스트를 텐서로 변환\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# CustomDataset 생성\n",
    "full_dataset = CustomDataset(dataset, labels)\n",
    "\n",
    "# 데이터셋 분할\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoader 생성\n",
    "batch_size = 320\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Flatten(),  # 50*50 이미지를 2500*1 벡터로 변환\n",
    "            nn.Linear(in_features=2500, out_features=1000, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1000, out_features=500, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=500, out_features=100, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features=2, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "# Hyperparameters\n",
    "training_epochs = 10\n",
    "k_folds = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# KFold 설정\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(full_dataset)):\n",
    "    print(f'FOLD {fold+1}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # DataLoader 생성\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "\n",
    "    train_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    test_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=test_subsampler)\n",
    "    \n",
    "    # 모델 초기화\n",
    "    model = MLP()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # 모델 학습\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for (images, labels) in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{training_epochs}], Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # 모델 평가\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on fold {fold+1}: {accuracy:.2f}%')\n",
    "    \n",
    "    # 결과 저장\n",
    "    fold_results.append(accuracy)\n",
    "\n",
    "# k-fold 결과 출력\n",
    "print('--------------------------------')\n",
    "print('K-FOLD CROSS VALIDATION RESULTS')\n",
    "print('--------------------------------')\n",
    "print(f'All fold results: {fold_results}')\n",
    "print(f'Average accuracy: {sum(fold_results) / len(fold_results):.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
